WEBVTT

1
00:00:00.007 --> 00:00:04.006
- Now, let's discuss the AI model. The algorithm.

2
00:00:04.006 --> 00:00:07.005
This is the brain of the operation.

3
00:00:07.005 --> 00:00:08.007
It defines how the model

4
00:00:08.007 --> 00:00:11.008
will learn patterns and relationships within the data,

5
00:00:11.008 --> 00:00:13.003
and then apply that knowledge

6
00:00:13.003 --> 00:00:16.001
to perform an intellectual task.

7
00:00:16.001 --> 00:00:18.001
In the case of generative models,

8
00:00:18.001 --> 00:00:21.005
it uses the knowledge it has gained to create new content

9
00:00:21.005 --> 00:00:25.002
in the form of images, text, video, audio, and more.

10
00:00:25.002 --> 00:00:27.007
With recent developments in AI, we are now

11
00:00:27.007 --> 00:00:30.008
experiencing a new foundational model

12
00:00:30.008 --> 00:00:33.000
that sets the basis for many AI systems

13
00:00:33.000 --> 00:00:34.008
that we'll use in the future.

14
00:00:34.008 --> 00:00:37.009
A foundational model is considered a new paradigm

15
00:00:37.009 --> 00:00:40.005
for building AI systems.

16
00:00:40.005 --> 00:00:41.008
It becomes a starting point

17
00:00:41.008 --> 00:00:44.009
for a wide range of downstream tasks.

18
00:00:44.009 --> 00:00:48.009
Looking back at the categorical evolution of AI systems,

19
00:00:48.009 --> 00:00:51.007
we started with the rule-based algorithms

20
00:00:51.007 --> 00:00:52.008
and those made decisions

21
00:00:52.008 --> 00:00:55.009
based on a set of explicit logical rules.

22
00:00:55.009 --> 00:00:59.009
That later evolved to search and optimization algorithms,

23
00:00:59.009 --> 00:01:03.005
that focused on efficiently exploring solution spaces

24
00:01:03.005 --> 00:01:06.000
and finding the optimal results.

25
00:01:06.000 --> 00:01:08.005
Think of a search engine.

26
00:01:08.005 --> 00:01:11.001
Then, we had machine learning algorithms,

27
00:01:11.001 --> 00:01:13.005
such as supervised, unsupervised,

28
00:01:13.005 --> 00:01:15.003
and reinforcement learning,

29
00:01:15.003 --> 00:01:18.004
which were used to analyze and classify data.

30
00:01:18.004 --> 00:01:21.001
Those really paved the way for massive progress

31
00:01:21.001 --> 00:01:25.008
in computer vision and natural language processing.

32
00:01:25.008 --> 00:01:27.004
Machine learning then branched

33
00:01:27.004 --> 00:01:29.008
into deep learning algorithms,

34
00:01:29.008 --> 00:01:32.001
which became the state of the art technology

35
00:01:32.001 --> 00:01:35.007
in image classification, object detection,

36
00:01:35.007 --> 00:01:38.005
and natural language understanding.

37
00:01:38.005 --> 00:01:41.001
GPT, the Generative Pre-trained Transform

38
00:01:41.001 --> 00:01:42.006
we discussed before,

39
00:01:42.006 --> 00:01:46.005
sets the ground for a new foundational model.

40
00:01:46.005 --> 00:01:48.009
It is a large language model

41
00:01:48.009 --> 00:01:52.001
that is trained on a massive amount of text data

42
00:01:52.001 --> 00:01:56.003
and is able to generate new text in response to user input.

43
00:01:56.003 --> 00:01:58.005
It is based on transformers,

44
00:01:58.005 --> 00:02:00.006
which is a deep learning architecture

45
00:02:00.006 --> 00:02:05.002
that was invented in 2017 and became very popular over time.

46
00:02:05.002 --> 00:02:07.005
In my previous course, I mentioned this

47
00:02:07.005 --> 00:02:10.003
as one of the most exciting developments in AI.

48
00:02:10.003 --> 00:02:13.003
Transformers are a type of neural network

49
00:02:13.003 --> 00:02:16.000
that uses a technique called self-attention

50
00:02:16.000 --> 00:02:19.006
to identify what parts of an input, sets of an article,

51
00:02:19.006 --> 00:02:21.006
are most essential,

52
00:02:21.006 --> 00:02:24.009
focusing on the relevant parts and ignoring the rest.

53
00:02:24.009 --> 00:02:29.003
This in turns helps the network better understand the input

54
00:02:29.003 --> 00:02:31.005
and makes more accurate predictions.

55
00:02:31.005 --> 00:02:33.002
Transformers are used in a variety

56
00:02:33.002 --> 00:02:35.002
of natural language tasks,

57
00:02:35.002 --> 00:02:37.007
from generating text based on input prompts,

58
00:02:37.007 --> 00:02:41.006
think about ChatGPT, to summarizing or translating text

59
00:02:41.006 --> 00:02:44.009
from one language to another with high accuracy.

60
00:02:44.009 --> 00:02:48.000
For example, I recently used GPT

61
00:02:48.000 --> 00:02:50.003
when I published a post in English

62
00:02:50.003 --> 00:02:52.005
and did a translation to Hindi

63
00:02:52.005 --> 00:02:54.006
and the result was incredible.

64
00:02:54.006 --> 00:02:57.002
Instead of simply translating the words,

65
00:02:57.002 --> 00:03:00.003
the model understood the intent of my post,

66
00:03:00.003 --> 00:03:02.003
and then rewrote that in Hindi,

67
00:03:02.003 --> 00:03:04.007
in the style of a native speaker.

68
00:03:04.007 --> 00:03:07.005
I got many compliments for my Hindi.

69
00:03:07.005 --> 00:03:10.006
Another recent foundational model is the diffusion model.

70
00:03:10.006 --> 00:03:12.006
Diffusion models have emerged in recent years

71
00:03:12.006 --> 00:03:15.005
as a powerful new approach to generative modeling.

72
00:03:15.005 --> 00:03:18.007
Specifically, in the field of image generation.

73
00:03:18.007 --> 00:03:22.000
They work by gradually transforming a starting image,

74
00:03:22.000 --> 00:03:25.004
usually starting from random noise, into a target image,

75
00:03:25.004 --> 00:03:26.007
like a photograph,

76
00:03:26.007 --> 00:03:31.002
through a series of small, randomly determined steps.

77
00:03:31.002 --> 00:03:34.003
Due to their ability to produce photorealistic images,

78
00:03:34.003 --> 00:03:36.004
diffusion models have potential applications

79
00:03:36.004 --> 00:03:38.006
in a wide range of domains.

80
00:03:38.006 --> 00:03:40.003
For instance, they could be used

81
00:03:40.003 --> 00:03:44.003
to accurately identify or classify objects in images,

82
00:03:44.003 --> 00:03:47.002
restore images, and generate rich media

83
00:03:47.002 --> 00:03:49.009
for use in many applications.

84
00:03:49.009 --> 00:03:53.002
Lastly, another promising area of AI research

85
00:03:53.002 --> 00:03:56.008
is called Reinforcement Learning from Human Feedback.

86
00:03:56.008 --> 00:03:59.004
This approach uses signals from human evaluators,

87
00:03:59.004 --> 00:04:01.006
such as upvotes or downvotes,

88
00:04:01.006 --> 00:04:04.005
to improve the performance of an AI model.

89
00:04:04.005 --> 00:04:06.006
By incorporating this feedback,

90
00:04:06.006 --> 00:04:09.002
the model learns to adjust its decisions

91
00:04:09.002 --> 00:04:12.005
based on how well they align with human expectations.

92
00:04:12.005 --> 00:04:15.000
This approach can be particularly effective in domains

93
00:04:15.000 --> 00:04:17.005
where it's difficult to define clear objective functions

94
00:04:17.005 --> 00:04:19.009
for the model to optimize for.

95
00:04:19.009 --> 00:04:23.003
For example, a social media moderation AI

96
00:04:23.003 --> 00:04:26.000
might use reinforcement learning from human feedback

97
00:04:26.000 --> 00:04:28.009
to better identify offensive or harmful content,

98
00:04:28.009 --> 00:04:31.006
and then limit its spread on the platform.

99
00:04:31.006 --> 00:04:34.003
Now, there's another very valuable tool

100
00:04:34.003 --> 00:04:38.002
for refining AI models, which is called prompt engineering,

101
00:04:38.002 --> 00:04:41.006
but the application of it goes far greater.

102
00:04:41.006 --> 00:04:44.000
It's a whole new interaction model,

103
00:04:44.000 --> 00:04:45.007
so let's learn about that next.
